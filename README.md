# Chatbot Web App with Pluggable LLM Backends

This is a Flask-based chatbot application that supports multiple LLM backends:
- Local models (TinyLlama, Mistral)
- Remote model API (e.g., via Flask server)
- OpenAI (e.g., GPT-4o)

## ğŸ“ Directory Structure

```
chatbot/
â”œâ”€â”€ app.py                     # Main Flask app
â”œâ”€â”€ chat_memory.json          # Stores chat history
â”œâ”€â”€ clear_chat_history.py     # Script to clear history
â”œâ”€â”€ start.sh                  # Startup script
â”œâ”€â”€ templates/                # HTML templates
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ login.html
â”‚   â””â”€â”€ register.html
â”œâ”€â”€ bot/
â”‚   â”œâ”€â”€ model_loader.py       # Dispatches to the correct model
â”‚   â”œâ”€â”€ openai_model.py       # GPT-4o via OpenAI API
â”‚   â”œâ”€â”€ remote_model.py       # For remote model API
â”‚   â”œâ”€â”€ mistral_model.py      # Local Mistral model
â”‚   â””â”€â”€ tinyllama_model.py    # Local TinyLlama model
â””â”€â”€ cloud/
    â”œâ”€â”€ cloud_model_server.py # Flask server to expose remote model
    â”œâ”€â”€ time_gpu_inference.py # Timing/debug script
    â”œâ”€â”€ start.sh              # Starts remote server
    â”œâ”€â”€ setup.sh              # Install CUDA + dependencies
    â”œâ”€â”€ mount_nvme.sh         # Mounts model storage
    â”œâ”€â”€ download_model.sh     # Download model from Hugging Face
    â”œâ”€â”€ download_model_from_s3.sh
    â”œâ”€â”€ upload_model_to_s3.sh
    â””â”€â”€ setup_model_server.sh # Run setup + start

## ğŸš€ How to Run

### Prerequisites
- Python 3.10+
- Optional GPU (CUDA) for local model inference
- OpenAI API key for GPT-4o

### ğŸ”¹ Local app (with TinyLlama or Mistral)

```bash
. ~/.venv/bin/activate
MODEL_NAME=tinyllama python app.py
# or
MODEL_NAME=mistral python app.py
```

### ğŸ”¹ Local app with OpenAI GPT-4o

```bash
. ~/.venv/bin/activate
OPENAI_API_KEY=your-key MODEL_NAME=openai python app.py
```

### ğŸ”¹ Local app with remote model (served on cloud)

```bash
. ~/.venv/bin/activate
MODEL_API_KEY=98765 MODEL_NAME=remote REMOTE_MODEL_URL=http://your-cloud-ip:5000/generate python app.py
```

## ğŸŒ©ï¸ Cloud Model Server Setup

On your cloud instance:
```bash
cd cloud
bash setup.sh
bash mount_nvme.sh
bash download_model.sh  # or download_model_from_s3.sh
bash start.sh
```

You can test performance using:

```bash
python time_gpu_inference.py
```

## ğŸ” Environment Variables

- `MODEL_NAME`: `tinyllama`, `mistral`, `openai`, or `remote`
- `MODEL_API_KEY`: used by remote model server for auth
- `REMOTE_MODEL_URL`: URL for cloud model endpoint
- `OPENAI_API_KEY`: your OpenAI API key

## ğŸ§  Features

- Memory/history-based conversation
- Dynamic model backend switching
- Summarization for long chats
- Markdown-formatted bot output
- Roleplay capability (if model supports it)

## ğŸ“ License

MIT License. Free to use and modify.